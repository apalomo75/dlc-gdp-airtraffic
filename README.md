# Data Life Cycle Project – GDP and Air Traffic Analysis

## Project overview
This repository contains the code and documentation for a group project developed within the *Data Life Cycle* course.  
The objective of the project is to analyse the relationship between economic activity (GDP) and air traffic across European countries using official open data sources.

The project follows a structured data life cycle approach, ensuring traceability, reproducibility, and proper preservation of the data transformation process.

## Authors
This project has been developed as a **group assignment**.

- Mario Díez Fernández
- Guillermo Sierra Díaz-Vargas
- David Pérez Caballero
- Jaime Terán Ramos
- Adrián Hernández Pérez
- Alejandro Palomo Morales  

## Data sources
The datasets used in this project are obtained from official and publicly available sources:

- **GDP data**: World Bank  
  - Indicator: *NY.GDP.MKTP.CD*  
  - Format: CSV  
  - Periodicity: Annual  

- **Air traffic data**: EUROCONTROL / ANS Performance  
  - Airport traffic statistics  
  - Format: CSV  
  - Periodicity: Annual  

The original datasets are not stored in this repository. Only the code and documentation required to reproduce the data pipeline are included.

## Repository structure
notebooks/
01_origin_to_bronze.ipynb
02_bronze_to_silver.ipynb
03_silver_to_gold.ipynb

modules/
utils.py

README.md

## Data pipeline

The data processing workflow is structured into three layers:

### Bronze layer
- Raw data directly downloaded from the original sources.
- No manual modification is applied.
- Versioning is handled through timestamps in file names.

### Silver layer
- Cleaned and standardised datasets.
- Temporal and structural harmonisation.
- Removal of unused variables and restriction of the temporal scope to the period of interest.

### Gold layer
- Final analytical dataset.
- Aggregated by country and year.
- GDP and air traffic data merged into a single table ready for analysis.

## Reproducibility

All data transformations are performed programmatically using Python notebooks.  
No manual editing of the datasets is carried out.

The final dataset can be fully regenerated by executing the notebooks in the following order:

1. `01_origin_to_bronze.ipynb`
2. `02_bronze_to_silver.ipynb`
3. `03_silver_to_gold.ipynb`

Common data persistence logic is centralised in reusable utility functions (`utils.py`), ensuring consistency across all pipeline stages.

## Preservation and versioning

- Open and widely supported formats (CSV with UTF-8 encoding) are used throughout the pipeline.
- Raw data are preserved separately from processed data.
- The repository provides version control for the code and documentation using Git.
- This approach supports long-term preservation of the data processing logic and facilitates reproducibility.

## License
The code and documentation in this repository are released under the **MIT License**.  
This permissive license allows reuse, modification, and redistribution of the code, fostering transparency and reproducibility in an academic context.

## Limitations
- No checksum or hash-based validation is implemented for the datasets.
- Data versioning relies on file naming conventions and folder structure.
- The project scope is academic and not intended for production deployment.
